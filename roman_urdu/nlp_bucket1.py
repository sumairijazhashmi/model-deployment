# -*- coding: utf-8 -*-
"""NLP_Bucket1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bPdJmtQczHgQ2tOCC0tGd6tecKt6Jb5e

# NLP Project
TODO: Add description and fancy text here

Urdu to Roman Urdu Transliterator

Paper:
https://sci-hub.se/10.1142/s0218001421520017

Help Reference:
https://www.tensorflow.org/addons/tutorials/networks_seq2seq_nmt

### This notebook contains the training, evaluation, and testing of the model for sentences of length <= 10
"""

# imports
import csv
import pickle
import tensorflow as tf
from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional
from tensorflow.keras.models import Model
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras import Input
from tensorflow.keras.layers import Concatenate, Attention
# !pip install tensorflow-addons
import tensorflow_addons as tfa
from tensorflow_addons.seq2seq import BasicDecoder
from keras.losses import SparseCategoricalCrossentropy
import time as time
import nltk
from nltk.translate.bleu_score import corpus_bleu, sentence_bleu
import matplotlib.pyplot as plt
import warnings
import random
# !pip install evaluate
import evaluate
# !pip install rouge_score
import numpy as np
import heapq


buckets_train_roman = None
buckets_train_urdu = None

buckets_test_roman = None
buckets_test_urdu = None

def load_data():
  # load datasets
  with open('./buckets_train_roman.csv', 'r') as f:
      reader = csv.reader(f)
      buckets_train_roman = [[[int(num) for num in sublist.split(',')] for sublist in row] for row in reader]

  with open('./buckets_train_urdu.csv', 'r') as f:
      reader = csv.reader(f)
      buckets_train_urdu = [[[int(num) for num in sublist.split(',')] for sublist in row] for row in reader]

  with open('./buckets_test_roman.csv', 'r') as f:
      reader = csv.reader(f)
      buckets_test_roman = [[[int(num) for num in sublist.split(',')] for sublist in row] for row in reader]

  with open('./buckets_test_urdu.csv', 'r') as f:
      reader = csv.reader(f)
      buckets_test_urdu = [[[int(num) for num in sublist.split(',')] for sublist in row] for row in reader]

  # just printing to see if i didnt mess up anything
  len(buckets_train_roman[0])

# ok, i didn't mess up anything - now time to build the tensorflow dataset for the model

current_index = 0 # this line is to be changed for each file

# hyperparameters for the dataset
buffer_size = 32000
batch_size = 64

def make_datasets():
  # make tensorflow train and test dataset from the buckets, shuffle the train data and convert all datasets to batches of size 64 (as done in the paper)
  train_dataset = tf.data.Dataset.from_tensor_slices((buckets_train_urdu[current_index], buckets_train_roman[current_index]))
  train_dataset = train_dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)

  test_dataset = tf.data.Dataset.from_tensor_slices((buckets_test_urdu[current_index], buckets_test_roman[current_index]))
  test_dataset = test_dataset.batch(batch_size, drop_remainder=True)

# load the tokenizers
with open('./tokenizer_roman.pkl', 'rb') as f:
    tokenizer_roman_string = f.read()

tokenizer_roman = pickle.loads(tokenizer_roman_string)

with open('./tokenizer_urdu.pkl', 'rb') as f:
    tokenizer_urdu_string = f.read()

tokenizer_urdu = pickle.loads(tokenizer_urdu_string)

# hyperparameters for the model
vocab_tar_size = len(tokenizer_roman.word_index) + 1
vocab_inp_size = len(tokenizer_urdu.word_index) + 1
embedding_dim = 256
units = 1024
steps_per_epoch = 17300

"""## Model Architecture:"""

# code used from the help reference and modified a bit to fit our needs

# encoder
class Encoder(Model):
  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz, num_layers=3):
      super(Encoder, self).__init__()
      self.batch_sz = batch_sz
      self.enc_units = enc_units
      self.embedding = Embedding(vocab_size, embedding_dim)
      self.num_layers = num_layers
      # bidirectional lstms, number of layers is passed from arguments
      self.lstms = [Bidirectional(LSTM(self.enc_units, return_sequences=True, return_state=True)) for i in range(self.num_layers)]

  # custom feedforward function
  def call(self, x, hidden):
    x = self.embedding(x)
    states = hidden
    new_states = []
    # for each bi_lstm layer, pass in the initial_state from the encoder's hidden state and then get the forward activation, concatenate backward and forward activation
    for i in range(self.num_layers):
        forward_init_state = states[i*4:i*4+2]
        backward_init_state = states[i*4+2:i*4+4]
        x, forward_h, forward_c, backward_h, backward_c = self.lstms[i](x, initial_state=forward_init_state + backward_init_state)
        h = Concatenate()([forward_h, backward_h])
        c = Concatenate()([forward_c, backward_c])
        new_states.extend([h, c])
    return x, new_states

  def build_initial_states(self, batch_sz):
      return [tf.zeros((batch_sz, self.enc_units)) for _ in range(self.num_layers * 4)]  # 4 initial states (2 for each lstm layer) per layer

# decoder
class Decoder(tf.keras.Model):
    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz, num_layers):
        super(Decoder, self).__init__()
        self.batch_sz = batch_sz
        self.dec_units = dec_units
        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)
        self.num_layers = num_layers
        # LSTM layers
        self.lstms = [tf.keras.layers.LSTM(self.dec_units, return_sequences=True, return_state=True) for _ in range(self.num_layers)]
        # Output layer
        self.fc = tf.keras.layers.Dense(vocab_size)
        # Attention layer
        self.attention = tf.keras.layers.Attention()

    def call(self, x, hidden, enc_output):
        # Prepare the query tensor for attention
        query = tf.expand_dims(hidden[0], 1)
        # Cast tensors to float32
        query = tf.cast(query, tf.float32)
        enc_output = tf.cast(enc_output, tf.float32)
        # Compute attention context vector
        context_vector = self.attention([query, enc_output])

        # Embed the input token
        x = self.embedding(x)
        # Concatenate context vector and embedded input
        x = tf.concat([context_vector, x], axis=-1)

        # Process through LSTM layers
        states = hidden
        for i in range(self.num_layers):
            x, h, c = self.lstms[i](x, initial_state=states[i*2:i*2+2])
            states[i*2:i*2+2] = [h, c]

        # Reshape output and apply fully connected layer
        output = tf.reshape(x, (-1, x.shape[2]))
        x = tf.cast(output, tf.float32)
        x = self.fc(x)

        return x, states

    def build_initial_states(self, batch_sz):
        return [tf.zeros((batch_sz, self.dec_units)) for _ in range(self.num_layers * 2)]

# class Decoder(Model):
#     def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz, num_layers):
#         super(Decoder, self).__init__()
#         self.batch_sz = batch_sz
#         self.dec_units = dec_units
#         self.embedding = Embedding(vocab_size, embedding_dim)
#         self.num_layers = num_layers
#         # lstm layers
#         self.lstms = [LSTM(self.dec_units, return_sequences=True, return_state=True) for _ in range(self.num_layers)]
#         # output layer
#         self.fc = Dense(vocab_size)
#         # attention layer
#         self.attention = Attention()

#     # feedforward function
#     def call(self, x, hidden, enc_output):
#         query = tf.expand_dims(hidden[0], 1)
#         context_vector = self.attention([tf.cast(query, tf.float32), tf.cast(enc_output, tf.float32)])

#         x = self.embedding(x)
#         x = tf.concat([context_vector, x], axis=-1)

#         states = hidden
#         for i in range(self.num_layers):
#             x, h, c = self.lstms[i](x, initial_state=states[i*2:i*2+2])
#             states[i*2:i*2+2] = [h, c]

#         output = tf.reshape(x, (-1, x.shape[2]))
#         x = self.fc(output)

#         return x, states
    
#         # query = tf.expand_dims(hidden[0], 1)
#         # context_vector = self.attention([query, enc_output])

#         # x = self.embedding(x)
#         # x = tf.concat([context_vector, x], axis=-1)

#         # states = hidden
#         # for i in range(self.num_layers):
#         #     x, h, c = self.lstms[i](x, initial_state=states[i*2:i*2+2])
#         #     states[i*2:i*2+2] = [h, c]

#         # output = tf.reshape(x, (-1, x.shape[2]))
#         # x = self.fc(output)

#         # return x, states

#     def build_initial_states(self):
#         return [tf.zeros((self.batch_sz, self.dec_units)) for _ in range(self.num_layers * 2)] #enc_hidden

# define Adam optimizer and use params given in paper
optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)

"""## Train and Evaluate Model"""

# define the loss function - code used from help reference
def loss_function(real, pred):
  # multi-class classification
  cross_entropy = SparseCategoricalCrossentropy(from_logits=True, reduction='none')
  loss = cross_entropy(y_true=real, y_pred=pred)
  # ignore the effect of padded 0s
  mask = tf.logical_not(tf.math.equal(real, 0))
  mask = tf.cast(mask, dtype=loss.dtype)
  loss = mask* loss
  loss = tf.reduce_mean(loss)
  return loss

# helper function to train each batch
@tf.function
def train_step(inp, targ, enc_hidden):
    loss = 0
    # pass input to encoder, get the output from the encoder and pass as input to the decoder
    with tf.GradientTape() as tape:
        enc_output, enc_hidden = encoder(inp, enc_hidden)
        dec_hidden = enc_hidden
        dec_input = tf.expand_dims([tokenizer_roman.word_index['<start>']] * batch_size, 1)

        for t in range(1, targ.shape[1]):
            predictions, dec_hidden = decoder(dec_input, dec_hidden, enc_output)
            loss += loss_function(targ[:, t], predictions)
            dec_input = tf.expand_dims(targ[:, t], 1)

        batch_loss = (loss / int(targ.shape[1]))
        # update training parameters (kind of like theta := theta + gradient)
        variables = encoder.trainable_variables + decoder.trainable_variables
        gradients = tape.gradient(loss, variables)
        optimizer.apply_gradients(zip(gradients, variables))

        return batch_loss

# # main training loop - code modified from help reference
# import time as time

# EPOCHS = 2
# train_loss = []
# num_layers = 1

# encoder = Encoder(vocab_inp_size, embedding_dim, units, batch_size, num_layers)
# decoder = Decoder(vocab_tar_size, embedding_dim, units*2, batch_size, num_layers)
# print('total batches:', len(train_dataset))
# for epoch in range(EPOCHS):
#     start = time.time()

#     enc_hidden = encoder.build_initial_states(batch_size)
#     total_loss = 0

#     for (batch, (inp, targ)) in enumerate(train_dataset.take(steps_per_epoch)):
#         batch_loss = train_step(inp, targ, enc_hidden)
#         train_loss.append(batch_loss)
#         total_loss += batch_loss

#         if batch % 100 == 0:
#             print(f'Epoch {epoch+1} Batch {batch // 100} Loss {batch_loss.numpy():.4f}')

#     print(f'Epoch {epoch+1} Loss {total_loss/steps_per_epoch:.4f}')
#     print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\n')

# encoder.save_weights('./encoder_weights.h5')
# decoder.save_weights('./decoder_weights.h5')

# # !jupyter nbconvert --to script config_template.ipynb

# # beam search:
# import math

def predict_beam(test_bucket):
  output = []
  # iterate through each example
  for i, (roman_sentence, urdu_sentence) in enumerate(test_bucket):
    # print(i, "examples done,", len(test_bucket) - i, "remain")
    # convert the sentence to its numeric sequence to give to the model
    urdu_sequence = [tokenizer_urdu.word_index[i] if i in tokenizer_urdu.word_index else tokenizer_urdu.word_index['<OOV>'] for i in urdu_sentence.split(' ')]
    urdu_sequence = pad_sequences([urdu_sequence], padding='post')
    # convert the sequence to a tensor
    urdu_sequence = tf.convert_to_tensor(urdu_sequence)
    inference_batch_size = urdu_sequence.shape[0]
    # call the encoder
    enc_start_state = [tf.zeros((inference_batch_size, units)) for _ in range(num_layers * 4)]
    enc_out, enc_hidden = encoder(urdu_sequence, enc_start_state)
    # create input for decoder
    dec_hidden = enc_hidden
    dec_input = tf.expand_dims([tokenizer_roman.word_index['<start>']] * inference_batch_size, 1)
    max_len_output = 10
    # get the input sequence for the decoder
    input_sequence = dec_input.numpy()
    # initialize the candidates from beam search, each row is of type: (candidate_sequence, log_probability)
    beam_candidates = [[input_sequence, 0.0]]
    beam_width = 5
    for t in range(max_len_output):
      # store the next candidates
      next_candidates = []
      # for each sequence in the current list of candidates
      for seq, score in beam_candidates:
        seq = tf.convert_to_tensor(seq)
        dec_input = tf.expand_dims(tf.convert_to_tensor([seq.numpy()[0][-1]]) if len(seq) > 0 else tokenizer_roman.word_index['<start>'], 0)
        # get the predictions for each token from the decoder for this sequence
        predictions, dec_hidden = decoder(dec_input, dec_hidden, enc_out)
        min_len = len(roman_sentence.split(' '))
        # punish printing <end> before len(roman_sentence)
        if t < min_len:
          end_token_inx = tokenizer_roman.word_index['<end>']
          mask = tf.one_hot(end_token_inx, predictions.shape[-1], dtype=tf.float32)
          predictions = tf.where(mask == 1, tf.ones_like(predictions) * -float('inf'), predictions)
        # store the top k most probable ones - beam search
        top_k_preds = tf.math.top_k(predictions[0], k=beam_width)
        for current_beam in range(beam_width):
          predicted_index = top_k_preds.indices[current_beam].numpy()
          candidate_sequence = tf.concat([seq, tf.convert_to_tensor([[predicted_index]])], axis=1)
          candidate_score = score - math.log(top_k_preds.values[current_beam])
          next_candidates.append([candidate_sequence, candidate_score])

      # sort candidates by their score
      sorted_candidates = sorted(next_candidates, key=lambda tup: tup[1])
      # get the top beam_width candidates
      beam_candidates = sorted_candidates[:beam_width]
      # exit loop if all candidates have generated <end> token
      # if all([tokenizer_roman.index_word[c[0].numpy()[0][-1]] == '<end>' for c in beam_candidates]):
      #   break

    # best candidate
    best_sequence = beam_candidates[0][0]
    result = ' '.join([tokenizer_roman.index_word[idx] for idx in best_sequence.numpy()[0]])
    output.append((roman_sentence.replace('<start>', '').replace('<end>', '').strip(), result.replace('<start>', '').replace('<end>', '').strip()))
  return output

# greedy search:
def predict_greedy(test_dataset):
  preds_bucket = []
  # for each example in the test set
  for i, tup in enumerate(test_dataset):
    # call the evaluate_sentence func on the urdu text's batch
    result = evaluate_sentence(tup[1]).replace("<start>", "").replace("<end>", "").strip()
    expected = tup[0].replace("<start>", "").replace("<end>", "").strip()
    preds_bucket.append((expected, result))
    if i % 100 == 0:
      print(i, "test examples done,", len(test_dataset) - i, "left.")
  print('all done!')
  return preds_bucket



def evaluate_sentence(sentence, encoder, decoder, num_layers):
    inputs = [tokenizer_urdu.word_index[i] if i in tokenizer_urdu.word_index else tokenizer_urdu.word_index['<OOV>'] for i in sentence.split(' ')]
    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], padding='post')
    inputs = tf.convert_to_tensor(inputs)
    inference_batch_size = inputs.shape[0]
    result = '<start>'
    enc_start_state = [tf.zeros((inference_batch_size, units)) for _ in range(num_layers * 4)]
    enc_out, enc_hidden = encoder(inputs, enc_start_state)

    dec_hidden = enc_hidden
    dec_input = tf.expand_dims([tokenizer_roman.word_index['<start>']] * inference_batch_size, 1)

    for t in range(40):

      predictions, dec_hidden = decoder(dec_input, dec_hidden, enc_out)

      # just use argmax to get the token with the highest probability
      predicted_id = tf.argmax(predictions, axis=-1).numpy()[0]

      result += tokenizer_roman.index_word[predicted_id] + ' '

      if tokenizer_roman.index_word[predicted_id] == '<end>':
          return result.strip()

      # pass the predicted token as the next input to the decoder
      dec_input = tf.expand_dims([predicted_id] * inference_batch_size, 1)

    return result

# helper function to calculate bleu score
def calculate_bleu_score(preds):
  # Split the dataset into expected translations and predicted translations.
  expected = [pair[0] for pair in preds]
  predicted = [pair[1] for pair in preds]

  # # Calculate the BLEU score for each sentence and average BLEU score for all test sentences
  bleu_scores = [sentence_bleu([ref.split()], pred.split()) for ref, pred in zip(predicted, expected)]
  avg_bleu_score = corpus_bleu([[ref.split()] for ref in expected], [pred.split() for pred in predicted])

  return avg_bleu_score * 100

def loss_graph(train_loss):
  train_steps = list(range(len(train_loss)))
  # Create the figure and axis objects
  fig, ax = plt.subplots()
  # Plot the data
  ax.plot(train_steps, train_loss, color='blue')
  # Set the title and axis labels
  ax.set_title('Training Loss')
  ax.set_xlabel('Training Steps')
  ax.set_ylabel('Training Loss')
  # Show the plot
  plt.show()

# plot the training loss graph
# loss_graph(train_loss)

# # load the test data
# import pickle

# with open('./test_buckets.pickle', 'rb') as f:
#     test_buckets = pickle.load(f)

# # calculate the prediction
# preds_bucket_beam = predict_beam(test_buckets[current_index][0:100])
# preds_bucket_beam

# # quantitaive analysis - bleu score of model, rouge score
# print('beam search')
# bleu_score = calculate_bleu_score(preds_bucket_beam)
# rouge = evaluate.load('rouge')
# expected = [pair[0] for pair in preds_bucket_beam]
# predicted = [pair[1] for pair in preds_bucket_beam]
# rouge_score = rouge.compute(predictions=predicted, references=expected)
# print("BLEU SCORE OF BUCKET SIZE 10:", bleu_score)
# print("ROUGE SCORE OF BUCKET SIZE 10:", rouge_score)

# # calculate the prediction
# preds_bucket_greedy = predict_greedy(test_buckets[current_index][0:100])
# preds_bucket_greedy

# # quantitaive analysis - bleu score of model, rouge score
# print('greedy search')
# bleu_score = calculate_bleu_score(preds_bucket_greedy)
# rouge = evaluate.load('rouge')
# expected = [pair[0] for pair in preds_bucket_greedy]
# predicted = [pair[1] for pair in preds_bucket_greedy]
# rouge_score = rouge.compute(predictions=predicted, references=expected)
# print("BLEU SCORE OF BUCKET SIZE 10:", bleu_score)
# print("ROUGE SCORE OF BUCKET SIZE 10:", rouge_score)

# # qualitative analysis - lets see a few example sentences and their transliterations
# # since greedy gave better performance on quant. metrics, we are just showing the sentences using greedy approach.
# for i in range(10):
#   inx = random.randint(0, len(preds_bucket_greedy))
#   r, u = test_buckets[current_index][inx]
#   print('Urdu Sentence:', u.replace('<start>', '').replace('<end>', '').strip())
#   print('Actual Roman Urdu Sentence:', r.replace('<start>', '').replace('<end>', '').strip())
#   orig, pred = preds_bucket_greedy[inx]
#   print('Predicted Roman Urdu Sentence:', pred.replace('<start>', '').replace('<end>', '').strip())
#   print()



