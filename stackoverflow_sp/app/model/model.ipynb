{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\pcf\\appdata\\roaming\\python\\python311\\site-packages (2.16.1)\n",
      "Requirement already satisfied: tensorflow-intel==2.16.1 in c:\\users\\pcf\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow) (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\pcf\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\pcf\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\pcf\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\pcf\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\pcf\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\pcf\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\pcf\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\users\\pcf\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\pcf\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\pcf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\pcf\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\pcf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\pcf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\pcf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\pcf\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\pcf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\pcf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\pcf\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.64.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\users\\pcf\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in c:\\users\\pcf\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\pcf\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\pcf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.24.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\pcf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: rich in c:\\users\\pcf\\appdata\\roaming\\python\\python311\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\pcf\\appdata\\roaming\\python\\python311\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\pcf\\appdata\\roaming\\python\\python311\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pcf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pcf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pcf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pcf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\pcf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.5)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\pcf\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\pcf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\pcf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\pcf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\pcf\\appdata\\roaming\\python\\python311\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (2.16.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\pcf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.1.2)\n",
      "Requirement already satisfied: swifter in c:\\users\\pcf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: pandas>=1.0.0 in c:\\users\\pcf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from swifter) (2.1.1)\n",
      "Requirement already satisfied: psutil>=5.6.6 in c:\\users\\pcf\\appdata\\roaming\\python\\python311\\site-packages (from swifter) (5.9.5)\n",
      "Requirement already satisfied: dask>=2.10.0 in c:\\users\\pcf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from dask[dataframe]>=2.10.0->swifter) (2023.10.1)\n",
      "Requirement already satisfied: tqdm>=4.33.0 in c:\\users\\pcf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from swifter) (4.66.1)\n",
      "Requirement already satisfied: click>=8.0 in c:\\users\\pcf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle>=1.5.0 in c:\\users\\pcf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (3.0.0)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in c:\\users\\pcf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (2023.10.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\pcf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (23.2)\n",
      "Requirement already satisfied: partd>=1.2.0 in c:\\users\\pcf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (1.4.1)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\pcf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (6.0.1)\n",
      "Requirement already satisfied: toolz>=0.10.0 in c:\\users\\pcf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (0.12.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in c:\\users\\pcf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (6.8.0)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\pcf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=1.0.0->swifter) (1.24.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\pcf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=1.0.0->swifter) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\pcf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=1.0.0->swifter) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\pcf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=1.0.0->swifter) (2023.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\pcf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.33.0->swifter) (0.4.6)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\pcf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from importlib-metadata>=4.13.0->dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (3.17.0)\n",
      "Requirement already satisfied: locket in c:\\users\\pcf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from partd>=1.2.0->dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (1.0.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pcf\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->swifter) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n",
    "!pip install swifter\n",
    "import swifter\n",
    "import tensorflow\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tensorflow\n",
      "Version: 2.16.1\n",
      "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
      "Home-page: https://www.tensorflow.org/\n",
      "Author: Google Inc.\n",
      "Author-email: packages@tensorflow.org\n",
      "License: Apache 2.0\n",
      "Location: C:\\Users\\PCF\\AppData\\Roaming\\Python\\Python311\\site-packages\n",
      "Requires: tensorflow-intel\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 256\n",
    "batch_size = 64\n",
    "num_epochs = 3\n",
    "max_words = 10000\n",
    "num_labels = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pretrain_bert(all_cloud, tokenizer, max_len, batch_size, num_epochs):\n",
    "#     model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     model.to(device)\n",
    "\n",
    "#     dataset = MyDataset(all_cloud, tokenizer, max_len)\n",
    "#     train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "#     optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    \n",
    "#     total_batches = len(train_loader)\n",
    "    \n",
    "#     for epoch in range(num_epochs):\n",
    "#         t = time.time()\n",
    "#         model.train()\n",
    "#         total_loss = 0.0\n",
    "#         for batch_idx, batch in enumerate(train_loader):\n",
    "#             input_ids = batch['input_ids'].to(device)\n",
    "#             attention_mask = batch['attention_mask'].to(device)\n",
    "#             labels = batch['labels'].to(device)\n",
    "            \n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "#             loss = outputs.loss\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             total_loss += loss.item()\n",
    "            \n",
    "#             if (batch_idx + 1) % 100 == 0:\n",
    "#                 batches_done = (epoch * total_batches) + (batch_idx + 1)\n",
    "#                 batches_left = num_epochs * total_batches - batches_done\n",
    "#                 print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{total_batches}], Loss: {loss.item()}, Time taken: {time.time() - t}')\n",
    "#                 print(f'{batches_done} batches done, {batches_left} batches left')\n",
    "\n",
    "#         total_loss /= total_batches\n",
    "#         print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss}, Time taken: {time.time() - t}')\n",
    "\n",
    "#     return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def merge_dfs(df1, df2):\n",
    "    merged = pd.concat([df1, df2], ignore_index=True)\n",
    "    return merged\n",
    "\n",
    "# find all SP keywords\n",
    "def get_sp_keywords(df):\n",
    "    df['keywords_found'] = df['keywords_found'].fillna('')\n",
    "    all_keywords = df['keywords_found'].str.split().explode()\n",
    "    unique_keywords = all_keywords.unique()\n",
    "    return unique_keywords\n",
    "\n",
    "# all rows from df where keyword is in title or tag\n",
    "def make_SP_train_dataset(df, SP_keywords):\n",
    "    df['processed_title'] = df['processed_title'].fillna('').astype(str)\n",
    "    SP_keywords_set = set(SP_keywords)\n",
    "\n",
    "    def row_contains_keyword(row):\n",
    "        title_words = row['processed_title'].split()\n",
    "        tag_words = row['processed_tags'].split()\n",
    "        title_contains = any(keyword in title_words for keyword in SP_keywords_set)\n",
    "        tags_contains = any(keyword in tag_words for keyword in SP_keywords_set)\n",
    "        return title_contains or tags_contains\n",
    "    \n",
    "    filtered_df = df[df.apply(row_contains_keyword, axis=1)]\n",
    "    return filtered_df\n",
    "\n",
    "# equal amount of posts from all cloud not in our relevant df\n",
    "def make_non_SP_train_dataset(big_df, SP_df):\n",
    "    non_SP_ids = set(big_df['id']) - set(SP_df['id'])\n",
    "    non_SP_df = big_df[big_df['id'].isin(non_SP_ids)]\n",
    "    non_SP_df = non_SP_df.sample(n=93178, random_state=42)  \n",
    "    return non_SP_df\n",
    "\n",
    "\n",
    "def add_labels(sp, nonsp):\n",
    "    sp = sp.assign(label='SP')\n",
    "    nonsp = nonsp.assign(label='nonSP')\n",
    "    return sp, nonsp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\PCF\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\PCF\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\PCF\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# Ensure that necessary NLTK datasets are downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Initialize Snowball stemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "# Regular expression patterns (same as before)\n",
    "code_snippet_pattern = re.compile(r'<code>.*?</code>')  # Detects code snippets\n",
    "url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')  # Detects URLs\n",
    "number_pattern = re.compile(r'\\b\\d+\\b')  # Detects standalone numbers\n",
    "extended_single_char_pattern = re.compile(r'\\b\\w\\b|\\w(?=\\d)|\\d(?=\\w)')  # Detects single characters and numbers attached to words\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove code snippets and HTML tags\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    code_elements = soup.find_all(['code', 'pre', 'a', 'img'])\n",
    "\n",
    "    # Remove code elements from the HTML\n",
    "    for code_element in code_elements:\n",
    "        code_element.decompose()\n",
    "\n",
    "    # Get the cleaned HTML without code snippets\n",
    "    text = soup.prettify()\n",
    "\n",
    "    text = BeautifulSoup(text, 'lxml').get_text()\n",
    "\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove URLs, numbers, and single characters\n",
    "    text = url_pattern.sub(' ', text)\n",
    "    text = number_pattern.sub(' ', text)\n",
    "    text = extended_single_char_pattern.sub(' ', text)\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords and non-alphabetic characters, apply stemming\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [stemmer.stem(word) if len(word) > 3 else word for word in tokens if word.isalpha() and word not in stop_words and word != '' and len(word) > 1]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def preprocess_tags(tags_text):\n",
    "    if pd.isnull(tags_text):  \n",
    "        return \"\"\n",
    "    tags_text = tags_text.replace(\"|\", \" \")\n",
    "    return preprocess_text(tags_text)\n",
    "\n",
    "def preprocess(df):\n",
    "    \n",
    "    for col in ['processed_title', 'processed_body', 'processed_tags']:\n",
    "        #  for col in ['processed_title', 'processed_body', 'processed_tags']:\n",
    "        df[col] = df[col].fillna('').astype(str)\n",
    "        new_col = f'processed_{col}'\n",
    "        df[new_col] = df[col].swifter.apply(preprocess_text)\n",
    "    \n",
    "    # Processing tags with the new function\n",
    "    # df['processed_tags'] = df['tags'].swifter.apply(preprocess_tags)\n",
    "    df['sumair_text'] = df['processed_title'] + ' ' + df['processed_body'] + ' ' + df['processed_tags']\n",
    "    df['sumair_text'] = df['sumair_text'].fillna('').astype(str)\n",
    "    df['sumair_text'] = df['sumair_text'].apply(lambda x: re.sub(r'\\s+', ' ', x).strip())\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sequences_and_labels(df):\n",
    "    texts = (df['sumair_text']).tolist()\n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
    "    label_to_int = {\"SP\": 1, \"nonSP\": 0}\n",
    "    df['label'] = df['label'].map(label_to_int)\n",
    "    return padded_sequences, tokenizer\n",
    "\n",
    "def training(padded_sequences, df, max_words, max_len, tokenizer):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(padded_sequences, df['label'], test_size=0.1, random_state=42)\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=max_words, output_dim=64, input_length=max_len),\n",
    "        Bidirectional(LSTM(64, return_sequences=True)),\n",
    "        Dropout(0.5),\n",
    "        Bidirectional(LSTM(32)),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f'Test Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "    model.save('C:\\\\Users\\\\PCF\\\\Downloads\\\\SO model\\\\model.h5')\n",
    "    with open('C:\\\\Users\\\\PCF\\\\Downloads\\\\SO model\\\\tokenizer.pickle', 'wb') as handle:\n",
    "        pickle.dump(tokenizer, handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # prepare data for model\n",
    "\n",
    "# def load_model_and_tokenizer(num_labels):\n",
    "#     tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "#     model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = num_labels)\n",
    "#     return model, tokenizer\n",
    "\n",
    "# def make_tokenized_dataset(df, tokenizer):\n",
    "#     df['processed_title'] = df['processed_title'].fillna('').astype(str)\n",
    "#     texts = (df['processed_title'] + df['processed_body'] + df['processed_tags']).tolist()\n",
    "#     tokenized_texts = [tokenizer.encode(text, add_special_tokens=True, max_length=256, truncation=True) for text in texts]\n",
    "#     padded_texts = [tokenized_text + [0] * (max_len - len(tokenized_text)) for tokenized_text in tokenized_texts]\n",
    "#     return padded_texts\n",
    "\n",
    "# def make_tensordataset(padded_texts, df):\n",
    "#     input_ids = torch.tensor(padded_texts)\n",
    "\n",
    "#     attention_masks = [[1 if token != 0 else 0 for token in text] for text in padded_texts]\n",
    "#     attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "#     labels = df['label'].tolist()\n",
    "#     unique_labels = list(set(labels)) \n",
    "#     label_map = {label: idx for idx, label in enumerate(unique_labels)}  \n",
    "#     label_ids = [label_map[label] for label in labels]\n",
    "#     labels_tensor = torch.tensor(label_ids)\n",
    "\n",
    "#     dataset = TensorDataset(input_ids, attention_masks, labels_tensor)\n",
    "\n",
    "#     return dataset\n",
    "        \n",
    "# def make_dataloaders(dataset):\n",
    "#     train_size = 0.8\n",
    "#     train_dataset, test_dataset = train_test_split(dataset, train_size=train_size, random_state=42)\n",
    "#     train_dataset, val_dataset = train_test_split(train_dataset, train_size=0.9, random_state=42)\n",
    "#     train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "#     test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "#     val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "#     return train_dataloader, val_dataloader, test_dataloader\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fine tune model\n",
    "# def fine_tuning(model, train_dataloader, val_dataloader):\n",
    "#     train_losses = []\n",
    "#     val_losses = []\n",
    "\n",
    "#     optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     model.to(device)\n",
    "\n",
    "#     num_epochs = 3\n",
    "#     for epoch in range(num_epochs):\n",
    "#         t = time.time()\n",
    "#         model.train()\n",
    "#         total_train_loss = 0.0\n",
    "#         for idx, batch in enumerate(train_dataloader):\n",
    "#             input_ids, attention_mask, lbls = batch\n",
    "#             input_ids, attention_mask, lbls = (\n",
    "#                 input_ids.to(device),\n",
    "#                 attention_mask.to(device),\n",
    "#                 lbls.to(device)\n",
    "#             )\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=lbls)\n",
    "#             loss = outputs.loss\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             total_train_loss += loss.item()\n",
    "\n",
    "#             if idx % 100 == 0:\n",
    "#                 print('training batch ', idx, ', time taken', time.time() - t)\n",
    "\n",
    "#         avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "#         train_losses.append(avg_train_loss)\n",
    "\n",
    "#         # validation\n",
    "#         model.eval()\n",
    "#         total_val_loss = 0.0\n",
    "#         for idx, val_batch in enumerate(val_dataloader):\n",
    "#             val_input_ids, val_attention_mask, val_labels = val_batch\n",
    "#             val_input_ids, val_attention_mask, val_labels = (\n",
    "#                 val_input_ids.to(device),\n",
    "#                 val_attention_mask.to(device),\n",
    "#                 val_labels.to(device)\n",
    "#             )\n",
    "\n",
    "#             with torch.no_grad():\n",
    "#                 outputs = model(input_ids=val_input_ids, attention_mask=val_attention_mask, labels=val_labels)\n",
    "\n",
    "#             val_loss = outputs.loss\n",
    "#             total_val_loss += val_loss.item()\n",
    "\n",
    "#             if idx % 100 == 0:\n",
    "#                 print('val batch ', idx, ', time taken', time.time() - t)\n",
    "\n",
    "#         avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "#         val_losses.append(avg_val_loss)\n",
    "\n",
    "#         print(f\"Epoch {epoch + 1}: Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "#     return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### self testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PCF\\AppData\\Local\\Temp\\ipykernel_19392\\1570682324.py:6: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  relevant_data = pd.read_csv('C:\\\\Users\\\\PCF\\\\Desktop\\\\sproj stuff\\\\datasets\\\\50_annotated_data.csv')\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "pre2018 = pd.read_csv(\"C:\\\\Users\\\\PCF\\\\Downloads\\\\SO model\\\\pre2018_processed.csv\")\n",
    "post2018 = pd.read_csv(\"C:\\\\Users\\\\PCF\\\\Downloads\\\\SO model\\\\post2018_processed.csv\")\n",
    "all_cloud = merge_dfs(pre2018, post2018)\n",
    "\n",
    "relevant_data = pd.read_csv('C:\\\\Users\\\\PCF\\\\Desktop\\\\sproj stuff\\\\datasets\\\\50_annotated_data.csv')\n",
    "relevant_ids = set(relevant_data['id'])\n",
    "relevant_df = all_cloud[all_cloud['id'].isin(relevant_ids)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keywords\n",
    "keywords = get_sp_keywords(relevant_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PCF\\AppData\\Local\\Temp\\ipykernel_19392\\509241967.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['processed_title'] = df['processed_title'].fillna('').astype(str)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8a0e71074e14a1ba63a5caf8441cb6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/186356 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9db0eb244b7a4a329d403e23b120616a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/186356 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a559fcf1c16347ae9216ce35d910417a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/186356 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SP_data = make_SP_train_dataset(relevant_df, keywords)\n",
    "SP_data = SP_data[['id', 'processed_title', 'processed_body', 'processed_tags']]\n",
    "\n",
    "non_SP_data = make_non_SP_train_dataset(all_cloud, relevant_data)\n",
    "\n",
    "SP_data, non_SP_data = add_labels(SP_data, non_SP_data)\n",
    "\n",
    "labeled_data = merge_dfs(SP_data, non_SP_data)\n",
    "\n",
    "labeled_data = preprocess(labeled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PCF\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4193/4193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m329s\u001b[0m 78ms/step - accuracy: 0.9290 - loss: 0.1797 - val_accuracy: 0.9950 - val_loss: 0.0247\n",
      "Epoch 2/10\n",
      "\u001b[1m4193/4193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m345s\u001b[0m 82ms/step - accuracy: 0.9958 - loss: 0.0210 - val_accuracy: 0.9965 - val_loss: 0.0164\n",
      "Epoch 3/10\n",
      "\u001b[1m4193/4193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m361s\u001b[0m 86ms/step - accuracy: 0.9964 - loss: 0.0153 - val_accuracy: 0.9956 - val_loss: 0.0220\n",
      "Epoch 4/10\n",
      "\u001b[1m4193/4193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m374s\u001b[0m 89ms/step - accuracy: 0.9975 - loss: 0.0111 - val_accuracy: 0.9964 - val_loss: 0.0221\n",
      "Epoch 5/10\n",
      "\u001b[1m4193/4193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m376s\u001b[0m 90ms/step - accuracy: 0.9984 - loss: 0.0068 - val_accuracy: 0.9964 - val_loss: 0.0271\n",
      "Epoch 6/10\n",
      "\u001b[1m4193/4193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m387s\u001b[0m 92ms/step - accuracy: 0.9988 - loss: 0.0053 - val_accuracy: 0.9963 - val_loss: 0.0229\n",
      "Epoch 7/10\n",
      "\u001b[1m4193/4193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m393s\u001b[0m 94ms/step - accuracy: 0.9987 - loss: 0.0047 - val_accuracy: 0.9963 - val_loss: 0.0353\n",
      "Epoch 8/10\n",
      "\u001b[1m4193/4193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m410s\u001b[0m 98ms/step - accuracy: 0.9993 - loss: 0.0027 - val_accuracy: 0.9960 - val_loss: 0.0322\n",
      "Epoch 9/10\n",
      "\u001b[1m4193/4193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m412s\u001b[0m 98ms/step - accuracy: 0.9996 - loss: 0.0014 - val_accuracy: 0.9959 - val_loss: 0.0349\n",
      "Epoch 10/10\n",
      "\u001b[1m4193/4193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m412s\u001b[0m 98ms/step - accuracy: 0.9998 - loss: 8.3255e-04 - val_accuracy: 0.9957 - val_loss: 0.0423\n",
      "\u001b[1m583/583\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 32ms/step - accuracy: 0.9961 - loss: 0.0363\n",
      "Test Accuracy: 99.53%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "padded_sequences, tokenizer = make_sequences_and_labels(labeled_data)\n",
    "training(padded_sequences, labeled_data, max_words, max_len, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
